{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import imageio\n",
    "#tf.enable_eager_execution()\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path=os.getcwd()+'\\\\\\sp-society-camera-model-identification'\n",
    "os.chdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.47058824, shape=(), dtype=float...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.6745098, shape=(), dtype=float3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.59607846, shape=(), dtype=float...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.41568628, shape=(), dtype=float...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.5764706, shape=(), dtype=float3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            location  \\\n",
       "0  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "1  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "2  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "3  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "4  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "\n",
       "                                               image  \n",
       "0  (((tf.Tensor(0.47058824, shape=(), dtype=float...  \n",
       "1  (((tf.Tensor(0.6745098, shape=(), dtype=float3...  \n",
       "2  (((tf.Tensor(0.59607846, shape=(), dtype=float...  \n",
       "3  (((tf.Tensor(0.41568628, shape=(), dtype=float...  \n",
       "4  (((tf.Tensor(0.5764706, shape=(), dtype=float3...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainpath = path+\"\\\\train\"\n",
    "os.chdir(trainpath)\n",
    "\n",
    "df1 = pd.read_pickle('train_images1.pkl')\n",
    "df1.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1375, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1375, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_pickle('train_images2.pkl')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2750, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1 = pd.concat([df1,df2])\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>image</th>\n",
       "      <th>camera</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.47058824, shape=(), dtype=float...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.6745098, shape=(), dtype=float3...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.59607846, shape=(), dtype=float...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.41568628, shape=(), dtype=float...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.5764706, shape=(), dtype=float3...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            location  \\\n",
       "0  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "1  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "2  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "3  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "4  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "\n",
       "                                               image    camera  \n",
       "0  (((tf.Tensor(0.47058824, shape=(), dtype=float...  HTC-1-M7  \n",
       "1  (((tf.Tensor(0.6745098, shape=(), dtype=float3...  HTC-1-M7  \n",
       "2  (((tf.Tensor(0.59607846, shape=(), dtype=float...  HTC-1-M7  \n",
       "3  (((tf.Tensor(0.41568628, shape=(), dtype=float...  HTC-1-M7  \n",
       "4  (((tf.Tensor(0.5764706, shape=(), dtype=float3...  HTC-1-M7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['camera'] = df1['location'].apply(lambda x: x.split('\\\\')[-2])\n",
    "df1.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HTC-1-M7': 0,\n",
       " 'iPhone-4s': 1,\n",
       " 'iPhone-6': 2,\n",
       " 'LG-Nexus-5x': 3,\n",
       " 'Motorola-Droid-Maxx': 4,\n",
       " 'Motorola-Nexus-6': 5,\n",
       " 'Motorola-X': 6,\n",
       " 'Samsung-Galaxy-Note3': 7,\n",
       " 'Samsung-Galaxy-S4': 8,\n",
       " 'Sony-NEX-7': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cameras = os.listdir()\n",
    "cameras.remove('.gitignore')\n",
    "cameras.remove('train_images1.pkl')\n",
    "cameras.remove('train_images2.pkl')\n",
    "cameras.remove('model_weights.h5')\n",
    "filecount = [len(os.listdir(camera))  for camera in cameras]\n",
    "cam = pd.DataFrame({'camera_type':cameras,'filecount':filecount})\n",
    "\n",
    "class_dict = {}\n",
    "for id in cam.index:\n",
    "    class_dict.update({cam.loc[id,'camera_type']:id})\n",
    "    \n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>image</th>\n",
       "      <th>camera</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.47058824, shape=(), dtype=float...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.6745098, shape=(), dtype=float3...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.59607846, shape=(), dtype=float...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.41568628, shape=(), dtype=float...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Study\\Capstone\\cameraDetection\\sp-society-c...</td>\n",
       "      <td>(((tf.Tensor(0.5764706, shape=(), dtype=float3...</td>\n",
       "      <td>HTC-1-M7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            location  \\\n",
       "0  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "1  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "2  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "3  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "4  D:\\Study\\Capstone\\cameraDetection\\sp-society-c...   \n",
       "\n",
       "                                               image    camera  class  \n",
       "0  (((tf.Tensor(0.47058824, shape=(), dtype=float...  HTC-1-M7      0  \n",
       "1  (((tf.Tensor(0.6745098, shape=(), dtype=float3...  HTC-1-M7      0  \n",
       "2  (((tf.Tensor(0.59607846, shape=(), dtype=float...  HTC-1-M7      0  \n",
       "3  (((tf.Tensor(0.41568628, shape=(), dtype=float...  HTC-1-M7      0  \n",
       "4  (((tf.Tensor(0.5764706, shape=(), dtype=float3...  HTC-1-M7      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['class'] = df1['camera'].apply(lambda x:class_dict[x])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['image_array'] = df1['image'].apply(lambda x:x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(df1['image_array'], df1['class'].values,test_size=.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "def reshape_input(Xtrain):\n",
    "    Xtrain = np.stack(Xtrain.values) #convert a pandas series of arrays into one single array\n",
    "    input_cols = Xtrain.shape[1]*Xtrain.shape[2]*Xtrain.shape[3]\n",
    "    Xtrain = np.reshape(Xtrain,(Xtrain.shape[0],input_cols)) #flatten the data to single set of neurons\n",
    "    return Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain= reshape_input(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#categorize output data. \n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def reshape_target(Ytrain):\n",
    "    Ytrain = to_categorical(Ytrain,num_classes=10)\n",
    "    return Ytrain\n",
    "\n",
    "Ytrain = reshape_target(Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(Xtrain,Ytrain,output_units=500):\n",
    "    model = Sequential()\n",
    "    input_cols = Xtrain.shape[1]\n",
    "    model.add(Dense(output_units, activation='relu',input_shape=(input_cols,)))\n",
    "    model.add(Dense(output_units))\n",
    "    model.add(Dense(output_units))\n",
    "    model.add(Dense(output_units))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1289 samples, validate on 553 samples\n",
      "Epoch 1/100\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 2.5107 - val_accuracy: 0.3237\n",
      "Epoch 2/100\n",
      "  64/1289 [>.............................] - ETA: 2s - loss: 0.0097 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyz\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_accurary` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.4735 - val_accuracy: 0.3237\n",
      "Epoch 3/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 2.5109 - val_accuracy: 0.3309\n",
      "Epoch 4/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 2.5131 - val_accuracy: 0.3291\n",
      "Epoch 5/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 2.5295 - val_accuracy: 0.3201\n",
      "Epoch 6/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.5305 - val_accuracy: 0.3291\n",
      "Epoch 7/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.4996 - val_accuracy: 0.3291\n",
      "Epoch 8/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 2.5107 - val_accuracy: 0.3255\n",
      "Epoch 9/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.5020 - val_accuracy: 0.3255\n",
      "Epoch 10/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 2.5221 - val_accuracy: 0.3183\n",
      "Epoch 11/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.5217 - val_accuracy: 0.3201\n",
      "Epoch 12/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.5247 - val_accuracy: 0.3255\n",
      "Epoch 13/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 2.5394 - val_accuracy: 0.3309\n",
      "Epoch 14/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 2.5475 - val_accuracy: 0.3237\n",
      "Epoch 15/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.5151 - val_accuracy: 0.3255\n",
      "Epoch 16/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.5424 - val_accuracy: 0.3273\n",
      "Epoch 17/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.5436 - val_accuracy: 0.3237\n",
      "Epoch 18/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 2.5532 - val_accuracy: 0.3237\n",
      "Epoch 19/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 2.5625 - val_accuracy: 0.3183\n",
      "Epoch 20/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.6016 - val_accuracy: 0.3255\n",
      "Epoch 21/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.5622 - val_accuracy: 0.3255\n",
      "Epoch 22/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 2.5424 - val_accuracy: 0.3201\n",
      "Epoch 23/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.6074 - val_accuracy: 0.3273\n",
      "Epoch 24/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.5648 - val_accuracy: 0.3237\n",
      "Epoch 25/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.6079 - val_accuracy: 0.3128\n",
      "Epoch 26/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.5644 - val_accuracy: 0.3165\n",
      "Epoch 27/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.5816 - val_accuracy: 0.3201\n",
      "Epoch 28/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.5871 - val_accuracy: 0.3165\n",
      "Epoch 29/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.5639 - val_accuracy: 0.3291\n",
      "Epoch 30/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 2.5680 - val_accuracy: 0.3255\n",
      "Epoch 31/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 2.5770 - val_accuracy: 0.3219\n",
      "Epoch 32/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.5799 - val_accuracy: 0.3255\n",
      "Epoch 33/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.5689 - val_accuracy: 0.3291\n",
      "Epoch 34/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.6037 - val_accuracy: 0.3237\n",
      "Epoch 35/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.5758 - val_accuracy: 0.3183\n",
      "Epoch 36/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.5891 - val_accuracy: 0.3237\n",
      "Epoch 37/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.5960 - val_accuracy: 0.3183\n",
      "Epoch 38/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.6283 - val_accuracy: 0.3128\n",
      "Epoch 39/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.5886 - val_accuracy: 0.3237\n",
      "Epoch 40/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.5873 - val_accuracy: 0.3219\n",
      "Epoch 41/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.5951 - val_accuracy: 0.3201\n",
      "Epoch 42/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.6189 - val_accuracy: 0.3237\n",
      "Epoch 43/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.6079 - val_accuracy: 0.3219\n",
      "Epoch 44/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 2.6312 - val_accuracy: 0.3219\n",
      "Epoch 45/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.6100 - val_accuracy: 0.3201\n",
      "Epoch 46/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.6100 - val_accuracy: 0.3201\n",
      "Epoch 47/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 2.6289 - val_accuracy: 0.3219\n",
      "Epoch 48/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.6115 - val_accuracy: 0.3219\n",
      "Epoch 49/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.6138 - val_accuracy: 0.3219\n",
      "Epoch 50/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.6245 - val_accuracy: 0.3219\n",
      "Epoch 51/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.6282 - val_accuracy: 0.3165\n",
      "Epoch 52/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.6384 - val_accuracy: 0.3165\n",
      "Epoch 53/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.6097 - val_accuracy: 0.3255\n",
      "Epoch 54/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.6208 - val_accuracy: 0.3291\n",
      "Epoch 55/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.6394 - val_accuracy: 0.3183\n",
      "Epoch 56/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.6139 - val_accuracy: 0.3255\n",
      "Epoch 57/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.6382 - val_accuracy: 0.3237\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.6464 - val_accuracy: 0.3183\n",
      "Epoch 59/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.6456 - val_accuracy: 0.3201\n",
      "Epoch 60/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 2.6467 - val_accuracy: 0.3201\n",
      "Epoch 61/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.6510 - val_accuracy: 0.3219\n",
      "Epoch 62/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.6461 - val_accuracy: 0.3219\n",
      "Epoch 63/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.6443 - val_accuracy: 0.3201\n",
      "Epoch 64/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.6463 - val_accuracy: 0.3219\n",
      "Epoch 65/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.6298 - val_accuracy: 0.3219\n",
      "Epoch 66/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.6634 - val_accuracy: 0.3201\n",
      "Epoch 67/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.6586 - val_accuracy: 0.3237\n",
      "Epoch 68/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.6523 - val_accuracy: 0.3219\n",
      "Epoch 69/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.6431 - val_accuracy: 0.3183\n",
      "Epoch 70/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.6564 - val_accuracy: 0.3201\n",
      "Epoch 71/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.6869 - val_accuracy: 0.3201\n",
      "Epoch 72/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.6670 - val_accuracy: 0.3237\n",
      "Epoch 73/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.6517 - val_accuracy: 0.3255\n",
      "Epoch 74/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.6570 - val_accuracy: 0.3237\n",
      "Epoch 75/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.6700 - val_accuracy: 0.3201\n",
      "Epoch 76/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.6613 - val_accuracy: 0.3201\n",
      "Epoch 77/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.6741 - val_accuracy: 0.3183\n",
      "Epoch 78/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.6737 - val_accuracy: 0.3201\n",
      "Epoch 79/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.6760 - val_accuracy: 0.3201\n",
      "Epoch 80/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.6818 - val_accuracy: 0.3219\n",
      "Epoch 81/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.6751 - val_accuracy: 0.3201\n",
      "Epoch 82/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.6654 - val_accuracy: 0.3219\n",
      "Epoch 83/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.6824 - val_accuracy: 0.3183\n",
      "Epoch 84/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.6827 - val_accuracy: 0.3219\n",
      "Epoch 85/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.6868 - val_accuracy: 0.3183\n",
      "Epoch 86/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.6878 - val_accuracy: 0.3183\n",
      "Epoch 87/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.6867 - val_accuracy: 0.3201\n",
      "Epoch 88/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.6818 - val_accuracy: 0.3183\n",
      "Epoch 89/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.6823 - val_accuracy: 0.3201\n",
      "Epoch 90/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.7040 - val_accuracy: 0.3273\n",
      "Epoch 91/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.6932 - val_accuracy: 0.3165\n",
      "Epoch 92/100\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.6955 - val_accuracy: 0.3165\n",
      "Epoch 93/100\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.6860 - val_accuracy: 0.3201\n",
      "Epoch 94/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.6940 - val_accuracy: 0.3219\n",
      "Epoch 95/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.7066 - val_accuracy: 0.3201\n",
      "Epoch 96/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.7052 - val_accuracy: 0.3237\n",
      "Epoch 97/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.6940 - val_accuracy: 0.3201\n",
      "Epoch 98/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.7349 - val_accuracy: 0.3201\n",
      "Epoch 99/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.7098 - val_accuracy: 0.3201\n",
      "Epoch 100/100\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.7075 - val_accuracy: 0.3201\n"
     ]
    }
   ],
   "source": [
    "#Testing a normal neural network\n",
    "\n",
    "earlystopping_monitor = EarlyStopping(monitor='val_accurary',min_delta=.1,patience=10,verbose=1)\n",
    "lr = [0.001]\n",
    "#changing the lr from .001 to .0001 to increase accuracy more than 32\n",
    "for l in lr:\n",
    "    model = get_model(Xtrain,Ytrain,1000)\n",
    "    model.load_weights('model_weights.h5')\n",
    "    sgd_optimizer = SGD(l)\n",
    "    model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(Xtrain,Ytrain,validation_split=.3,epochs=100,callbacks=[earlystopping_monitor])\n",
    "model.save_weights('model_weights.h5')\n",
    "#using lr of .001 has the best improvement in accuracy so far(.17 to .48 over 10 epoches) So, going lesser than that and increaseing the epoches to 25\n",
    "#.00001 is slow as it increases accuracy from.10 to .27 in 25 epoches\n",
    "#.0001 .13->.46 in 25 epoches (caused resouorceexhaustedError)\n",
    "#.001 .17 -> .72 (it fluctuates after .71, maxes at .72 and decreases again). \n",
    "#Increasing the output units to 1000 to see the increase, with lr of .001. Accuracy reaches upto .76 but its not converging. \n",
    "#WOULD REDUCING LEARNING RATE AFTER 20 EPOCHES HELP TO CONVEREGE?\n",
    "#validation accuracy is still at .28 max\n",
    "\n",
    "#to use Model checkpoint, refer link here: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "#preprocessing and training\n",
    "#training->hyperparameter\n",
    "#preprocessing -> rotate imags and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3307.9921875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, psutil\n",
    "def usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info()[0] / float(2 ** 20)\n",
    "usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.contrib.memory_stats import BytesInUse\n",
    "#with tensorflow.device('/device:GPU:0'):  # Replace with device you are interested in\n",
    "  #bytes_in_use = BytesInUse()\n",
    "#print(bytes_in_use)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
